## Introduction

<p> The company under study has a marketing department that has been tracking fourteen different variables on clients who purchased their new life insurance product. The main objective is to determine if the data collected can be effectively used to predict the likelihood that a new customer will buy an insurance product. This issue is significant for the organization since it may help them refine their marketing efforts and more effectively target potential clients. The data set provided contains a response variable that indicates whether a customer has purchased a life insurance product, and this type of data is well suited for supervised learning machine learning algorithms, which can use labelled data to train a model that can predict the purchasing behaviour of new customers. </p>

## Data cleaning and pre-processing

<p> The data cleaning process started by identifying missing values in the dataset. The 'education' column had only 1.85% missing values, so mode imputation was applied. However, the 'marriage' and 'house_owner' variables had more significant shares of missing values, with 35% and 9% missing values, respectively, therefore, to avoid inaccuracy in imputation, missing values were replaced with a new category called 'Unknown'. The approach to missing value treatment ensured that all 40000 observations were preserved in the dataset without any loss, and by doing so the response variable class balance was maintained at initial 50/50 proportion. </p>
<p> Next, typos present in the 'child' variable were identified and fixed. The '0' category was allegedly associated with not having kids ('N' category), so the '0' values were replaced with 'N.' </p>
<p> For encoding categorical variables, integer encoding was used for variables that have natural ordering in them. In cases where missing values were present, they were coded as 0 with the creation of a separate binary variable indicating missing information. One-hot encoding was used for categorical variables with three or more categories, and dummy encoding for variables with only two categories. </p>
<p> Feature engineering was performed by creating two new features. The first feature involved binning the continuous 'house_val' variable into discrete categories (none, low, below median, above median, and high) using percentiles as cut-off points, which addressed the significant number of outliers in the data. The grouping was done inside each ‘region’ to account for differences in house prices across different geographies. The second feature was 'income to house value ratio,' which was calculated by dividing the family income bracket by the binned house value variable. The feature helped to identify customers that likely have more disposable income. </p>

## Modelling methodology

<p> After cleaning the data, the next step is to build classification algorithms to predict the response variable. The first algorithm implemented was <b>Logistic Regression</b>. <b>Recursive Feature Elimination with Cross-Validation (RFECV)</b> was used to select a subset of features with <b>StratifiedKFold cross-validation</b> to ensure that the class distribution was balanced in each fold. However, since RFECV does not consider statistical significance of predictors, so backward stepwise variable selection was performed after to remove statistically insignificant variables from the set of variables generated by it. The remaining subset of variables was then used to predict the response variable in the entire dataset by using 10-fold cross-validation. </p>
<p>Next, tree-based algorithms such as <b>Decision Tree</b>, <b>Random Forest</b>, and <b>Gradient Boosting</b> were implemented. <b>GridSearchCV</b> approach within ‘sklearn’ library was used to perform hyperparameter optimization. For Decision Tree, the function to measure the quality of a split, maximum depth of the decision tree, and the minimum number of samples required to split an internal node were optimized. For Random Forest, the number of trees, the function to measure the quality of a split, maximum depth of each decision tree, and the number of features to consider during each split were optimized. For Gradient Boosting, the same set of parameters as Random Forest were optimized, plus the learning rate, which is a step size used to update the weights.</p>
<p>After optimization, the sets with best parameters were used to run the models and predict the response variables using cross-validation in the similar manner. After applying the Random Forest algorithm, the feature importance scores were extracted to identify the most influential predictors in determining the likelihood of a client purchasing life insurance. This information can be used to gain insight into which variables are the most important in the prediction model and to potentially inform future data collection strategies.</p>
<p>Additionally, <b>Naive Bayes</b> and <b>k-Nearest Neighbours</b> classifiers were attempted as well to compare their performance to the other algorithms and ensure no additional predictive performance could be gained. The performance of each proposed algorithm was assessed using ROC curve with AUC values, accuracy, precision, recall, and F1 scores.</p>


